#!/usr/bin/env python3
"""Generate Pydantic models from JSON Schema.

This script is the source of truth workflow for artifact models.
Run after modifying any schema in schemas/*.schema.json.

Usage:
    uv run python scripts/generate_models.py
    uv run python scripts/generate_models.py --schemas-dir /path/to/schemas
    uv run python scripts/generate_models.py --output-file /path/to/generated.py

The generated file will be written to:
    src/questfoundry/artifacts/generated.py

Exit codes:
    0: Generation successful
    1: Generation failed (schema error, ruff error, etc.)
"""

from __future__ import annotations

import argparse
import json
import keyword
import subprocess
import sys
from pathlib import Path
from typing import Any

# Default paths (relative to script location)
DEFAULT_OUTPUT = Path(__file__).parent.parent / "src/questfoundry/artifacts/generated.py"
DEFAULT_SCHEMAS_DIR = Path(__file__).parent.parent / "schemas"

# Header for generated file
GENERATED_HEADER = '''\
"""Auto-generated Pydantic models from JSON Schema.

DO NOT EDIT THIS FILE DIRECTLY.
Run `uv run python scripts/generate_models.py` to regenerate.

Source schemas: schemas/*.schema.json
"""

from __future__ import annotations

from typing import Annotated, Literal

from pydantic import BaseModel, Field, StringConstraints

'''


def safe_field_name(name: str) -> tuple[str, str | None]:
    """Convert a field name to a safe Python identifier.

    Args:
        name: The original field name from the schema.

    Returns:
        Tuple of (python_name, alias_or_none). If the name is a Python keyword,
        returns (name_, name). Otherwise returns (name, None).
    """
    if keyword.iskeyword(name):
        return f"{name}_", name
    return name, None


def snake_to_pascal(name: str) -> str:
    """Convert snake_case to PascalCase for class naming.

    Args:
        name: A snake_case string (e.g., "content_notes").

    Returns:
        PascalCase string (e.g., "ContentNotes").
    """
    return "".join(word.capitalize() for word in name.split("_"))


def escape_description(desc: str) -> str:
    """Escape special characters in description strings for Python code.

    Args:
        desc: The description string from the schema.

    Returns:
        Escaped string safe for use in generated Python code.
    """
    return (
        desc.replace("\\", "\\\\")
        .replace('"', '\\"')
        .replace("\n", "\\n")
        .replace("\r", "\\r")
        .replace("\t", "\\t")
    )


def format_default_value(value: Any) -> str:
    """Format a JSON Schema default value as Python code.

    Args:
        value: The default value from the schema.

    Returns:
        Python code representation of the value.
    """
    if isinstance(value, str):
        escaped = value.replace("\\", "\\\\").replace('"', '\\"')
        return f'"{escaped}"'
    elif isinstance(value, bool):
        return "True" if value else "False"
    elif value is None:
        return "None"
    else:
        # int, float, etc.
        return str(value)


def schema_to_python_type(
    prop: dict[str, Any],
    prop_name: str,
    parent_required: set[str],
) -> tuple[str, str]:
    """Convert JSON Schema property to Python type annotation and Field.

    Args:
        prop: The property schema definition.
        prop_name: Name of the property.
        parent_required: Set of required field names from parent object.

    Returns:
        Tuple of (type_annotation, field_definition).
    """
    prop_type = prop.get("type")
    description = escape_description(prop.get("description", ""))

    # Handle const (Literal type)
    if "const" in prop:
        const_val = prop["const"]
        const_val_str = format_default_value(const_val)
        return f"Literal[{const_val_str}]", const_val_str

    # Handle integer
    if prop_type == "integer":
        field_args = []
        if description:
            field_args.append(f'description="{description}"')
        if "minimum" in prop:
            field_args.append(f"ge={prop['minimum']}")

        is_required = prop_name in parent_required
        has_default = "default" in prop

        if has_default:
            # Use schema-specified default
            default_val = format_default_value(prop["default"])
            field_args.insert(0, f"default={default_val}")
            type_anno = "int"
        elif not is_required:
            type_anno = "int | None"
            field_args.insert(0, "default=None")
        else:
            type_anno = "int"

        field_def = f"Field({', '.join(field_args)})" if field_args else "..."
        return type_anno, field_def

    # Handle string
    if prop_type == "string":
        field_args = []
        if description:
            field_args.append(f'description="{description}"')
        if prop.get("minLength"):
            field_args.append(f"min_length={prop['minLength']}")

        is_required = prop_name in parent_required
        has_default = "default" in prop

        if has_default:
            default_val = format_default_value(prop["default"])
            field_args.insert(0, f"default={default_val}")
            type_anno = "str"
        elif not is_required:
            type_anno = "str | None"
            field_args.insert(0, "default=None")
        else:
            type_anno = "str"

        field_def = f"Field({', '.join(field_args)})" if field_args else "..."
        return type_anno, field_def

    # Handle array
    if prop_type == "array":
        items = prop.get("items", {})
        item_type = items.get("type")
        if not item_type:
            raise ValueError(
                f"Array field '{prop_name}' is missing 'items.type'. "
                "All array fields must specify the type of their items."
            )

        # Map JSON Schema types to Python types for array items
        json_to_python_type = {
            "string": "str",
            "integer": "int",
        }

        # Check if items have minLength constraint
        if item_type == "string":
            item_min_length = items.get("minLength")
            if item_min_length:
                item_type_anno = f"Annotated[str, StringConstraints(min_length={item_min_length})]"
            else:
                item_type_anno = "str"
        elif item_type in json_to_python_type:
            item_type_anno = json_to_python_type[item_type]
        else:
            raise ValueError(
                f"Unsupported array item type '{item_type}' for field '{prop_name}'. "
                f"Supported types: {set(json_to_python_type.keys())}"
            )

        field_args = []
        if description:
            field_args.append(f'description="{description}"')
        if prop.get("minItems"):
            field_args.append(f"min_length={prop['minItems']}")

        is_required = prop_name in parent_required
        if not is_required:
            field_args.insert(0, "default_factory=list")

        type_anno = f"list[{item_type_anno}]"
        field_def = f"Field({', '.join(field_args)})" if field_args else "..."
        return type_anno, field_def

    # Handle nested object (will be a reference to another class)
    if prop_type == "object":
        # Generate class name from property name
        class_name = snake_to_pascal(prop_name)

        is_required = prop_name in parent_required
        if not is_required:
            type_anno = f"{class_name} | None"
            default = "default=None"
        else:
            type_anno = class_name
            default = ""

        field_args = []
        if default:
            field_args.append(default)
        if description:
            field_args.append(f'description="{description}"')

        field_def = f"Field({', '.join(field_args)})" if field_args else "..."
        return type_anno, field_def

    # Unsupported type - raise explicit error
    supported_types = {"integer", "string", "array", "object"}
    raise ValueError(
        f"Unsupported JSON Schema type '{prop_type}' for field '{prop_name}'. "
        f"Supported types: {supported_types}"
    )


def generate_nested_class(
    prop_name: str,
    prop_schema: dict[str, Any],
) -> str:
    """Generate a nested Pydantic model class.

    Args:
        prop_name: Property name (used for class naming).
        prop_schema: The object schema definition.

    Returns:
        Python class definition as string.
    """
    class_name = snake_to_pascal(prop_name)
    description = prop_schema.get("description", f"{class_name} model.")
    properties = prop_schema.get("properties", {})
    required = set(prop_schema.get("required", []))

    lines = [f"class {class_name}(BaseModel):"]
    lines.append(f'    """{description}"""')
    lines.append("")

    # Handle empty properties - need explicit pass for valid Python
    if not properties:
        lines.append("    pass")
        lines.append("")
        return "\n".join(lines)

    # Sort properties for deterministic output
    for field_name in sorted(properties.keys()):
        field_schema = properties[field_name]
        type_anno, field_def = schema_to_python_type(field_schema, field_name, required)

        # Handle Python keywords
        py_name, alias = safe_field_name(field_name)
        if alias:
            # Inject alias into Field()
            if field_def == "...":
                field_def = f'Field(alias="{alias}")'
            elif field_def.startswith("Field("):
                field_def = field_def.replace("Field(", f'Field(alias="{alias}", ', 1)

        lines.append(f"    {py_name}: {type_anno} = {field_def}")

    lines.append("")
    return "\n".join(lines)


def generate_artifact_class(
    schema: dict[str, Any],
    artifact_type: str,
) -> tuple[str, list[str]]:
    """Generate the main artifact Pydantic model class.

    Args:
        schema: The full JSON schema.
        artifact_type: Type identifier (e.g., "dream").

    Returns:
        Tuple of (class_definition, list_of_nested_class_definitions).
    """
    title = schema.get("title", f"{artifact_type.upper()} Artifact")
    description = schema.get("description", f"{title} model.")
    properties = schema.get("properties", {})
    required = set(schema.get("required", []))

    class_name = f"{snake_to_pascal(artifact_type)}Artifact"
    nested_classes: list[str] = []

    lines = [f"class {class_name}(BaseModel):"]
    lines.append(f'    """{title} - {description}"""')
    lines.append("")

    # Handle empty properties - need explicit pass for valid Python
    if not properties:
        lines.append("    pass")
        lines.append("")
        return "\n".join(lines), nested_classes

    # Sort properties for deterministic output
    for field_name in sorted(properties.keys()):
        field_schema = properties[field_name]

        # Handle nested objects - generate separate class
        if field_schema.get("type") == "object" and "properties" in field_schema:
            nested_class = generate_nested_class(field_name, field_schema)
            nested_classes.append(nested_class)

        type_anno, field_def = schema_to_python_type(field_schema, field_name, required)

        # Handle Python keywords
        py_name, alias = safe_field_name(field_name)
        if alias:
            # Inject alias into Field()
            if field_def == "...":
                field_def = f'Field(alias="{alias}")'
            elif field_def.startswith("Field("):
                field_def = field_def.replace("Field(", f'Field(alias="{alias}", ', 1)

        lines.append(f"    {py_name}: {type_anno} = {field_def}")

    lines.append("")
    return "\n".join(lines), nested_classes


def generate_from_schema(schema_path: Path) -> str:
    """Generate Python code from a JSON schema file.

    Args:
        schema_path: Path to the JSON schema file.

    Returns:
        Generated Python code as string.
    """
    with schema_path.open() as f:
        schema = json.load(f)

    # Extract artifact type from filename (e.g., "dream" from "dream.schema.json")
    artifact_type = schema_path.stem.replace(".schema", "")

    artifact_class, nested_classes = generate_artifact_class(schema, artifact_type)

    # Nested classes come first (dependencies)
    code_parts = [*nested_classes, artifact_class]
    return "\n".join(code_parts)


def format_with_ruff(file_path: Path) -> bool:
    """Format generated file with ruff.

    Args:
        file_path: Path to the file to format.

    Returns:
        True if formatting succeeded, False otherwise.
    """
    try:
        # Run ruff format
        result = subprocess.run(
            ["uv", "run", "ruff", "format", str(file_path)],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            print(f"Ruff format failed: {result.stderr}", file=sys.stderr)
            return False

        # Run ruff check with fixes
        result = subprocess.run(
            ["uv", "run", "ruff", "check", "--fix", str(file_path)],
            capture_output=True,
            text=True,
            check=False,
        )
        if result.returncode != 0:
            print(f"Ruff check failed: {result.stderr}", file=sys.stderr)
            return False

        return True
    except FileNotFoundError:
        print("Error: ruff not found - cannot format generated code", file=sys.stderr)
        return False


def parse_args() -> argparse.Namespace:
    """Parse command-line arguments.

    Returns:
        Parsed arguments namespace.
    """
    parser = argparse.ArgumentParser(
        description="Generate Pydantic models from JSON Schema files.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--schemas-dir",
        type=Path,
        default=DEFAULT_SCHEMAS_DIR,
        help=f"Directory containing *.schema.json files (default: {DEFAULT_SCHEMAS_DIR})",
    )
    parser.add_argument(
        "--output-file",
        type=Path,
        default=DEFAULT_OUTPUT,
        help=f"Output file path (default: {DEFAULT_OUTPUT})",
    )
    return parser.parse_args()


def main() -> int:
    """Generate models from all schema files.

    Returns:
        Exit code (0 for success, 1 for failure).
    """
    args = parse_args()
    schemas_dir: Path = args.schemas_dir
    output_file: Path = args.output_file

    # Find all schema files
    schema_files = sorted(schemas_dir.glob("*.schema.json"))
    if not schema_files:
        print(f"No schema files found in {schemas_dir}", file=sys.stderr)
        return 1

    print(f"Found {len(schema_files)} schema file(s)")

    # Generate code for each schema
    all_code = [GENERATED_HEADER]
    artifact_types: list[str] = []

    for schema_path in schema_files:
        print(f"  Processing {schema_path.name}...")
        try:
            code = generate_from_schema(schema_path)
            all_code.append(code)
            artifact_type = schema_path.stem.replace(".schema", "")
            artifact_types.append(f"{snake_to_pascal(artifact_type)}Artifact")
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"Error processing {schema_path}: {e}", file=sys.stderr)
            return 1

    # Add type alias for all artifact types
    if artifact_types:
        all_code.append("# Type alias for artifact types")
        all_code.append(f"ArtifactType = {' | '.join(artifact_types)}")
        all_code.append("")

    # Write generated file
    generated_code = "\n".join(all_code)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(generated_code)
    print(f"Generated {output_file}")

    # Format with ruff
    if not format_with_ruff(output_file):
        print("Warning: ruff formatting failed", file=sys.stderr)
        return 1

    print("Done!")
    return 0


if __name__ == "__main__":
    sys.exit(main())
